{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generative Models for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1 = open('book1.txt', 'r', encoding=\"utf8\").read().lower()[:10000]\n",
    "book2 = open('book2.txt', 'r', encoding=\"utf8\").read().lower()[:10000]\n",
    "book3 = open('book3.txt', 'r', encoding=\"utf8\").read().lower()[:10000]\n",
    "book4 = open('book4.txt', 'r', encoding=\"utf8\").read().lower()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length 40003\n"
     ]
    }
   ],
   "source": [
    "text = book1 + '\\n' + book2 + '\\n' + book3 + '\\n' + book4\n",
    "print('Text Length', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Characters to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Characters 55\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('Total Unique Characters', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up into subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Sequences 39904\n"
     ]
    }
   ],
   "source": [
    "maxlen = 99\n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i:i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('No. of Sequences', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    #Helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    #Function invoked at the end of each epoch. Prints generated Text.\n",
    "    #print('\\n----- Generating Text after Epoch: %d'%epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        #print('----- Diversity:', diversity)\n",
    "        \n",
    "        generated = ''\n",
    "        sentence = text[start_index : start_index + maxlen]\n",
    "        generated += sentence\n",
    "        #print('----- Generating with seed:\"'+sentence+'\"')\n",
    "        #sys.stdout.write(generated)\n",
    "        \n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1\n",
    "            \n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            \n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "            \n",
    "            #sys.stdout.write(next_char)\n",
    "            #sys.stdout.flush()\n",
    "        #print('\\n')\n",
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = 'weights.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                            verbose=1, save_best_only=True, \n",
    "                            mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                             patience=1, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [print_callback, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "39904/39904 [==============================] - 294s 7ms/step - loss: 1.5360\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.53596, saving model to weights.hdf5\n",
      "Epoch 2/5\n",
      "39904/39904 [==============================] - 289s 7ms/step - loss: 1.5132\n",
      "\n",
      "Epoch 00002: loss improved from 1.53596 to 1.51318, saving model to weights.hdf5\n",
      "Epoch 3/5\n",
      "39904/39904 [==============================] - 240s 6ms/step - loss: 1.4985\n",
      "\n",
      "Epoch 00003: loss improved from 1.51318 to 1.49850, saving model to weights.hdf5\n",
      "Epoch 4/5\n",
      "39904/39904 [==============================] - 241s 6ms/step - loss: 1.4885\n",
      "\n",
      "Epoch 00004: loss improved from 1.49850 to 1.48852, saving model to weights.hdf5\n",
      "Epoch 5/5\n",
      "39904/39904 [==============================] - 238s 6ms/step - loss: 1.4694\n",
      "\n",
      "Epoch 00005: loss improved from 1.48852 to 1.46940, saving model to weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c2193a1f98>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=20, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length, diversity):\n",
    "    #Get random starting Text\n",
    "    #start_index = random.randint(0, len(text)-maxlen-1)\n",
    "    generated = ''\n",
    "    sentence = 'There are those who take mental phenomena naively, just as they would physical phenomena. This scho'.lower()\n",
    "    #sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are those who take mental phenomena naively, just as they would physical phenomena. this schoold the table and the table and considert to a strong and the table of the table of the same to and the same to show the same to the same was the same that the same was in the same that the table of the sense of and the same to a more are and the table of the table and man that the table of the same was a statement and the same to a certain the same and and the same was we see that the table is the same and the same to the same was in the table and the same was the table and the table and the colour that the table is the same with we see that the table of the table of the table and any of the sense and and at the same man consided in the same was an other and and the same to a colour the same to a strange and the same to a sense and and the science of the sense of the same world of the same most to a sense and and at the same man and the are in the same mover and any that the table of the sense of the are and the table and any of the table and any of the sense of the same was the same \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(1000, 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
