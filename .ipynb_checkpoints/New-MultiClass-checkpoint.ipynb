{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage\n",
    "from skimage import io\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "from skimage import img_as_ubyte\n",
    "plt.rcParams['font.size'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper import helper_classes as hc\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters.rank import median\n",
    "from skimage.morphology import disk\n",
    "from skimage import data, io, filters\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import transform\n",
    "from skimage.filters.rank import enhance_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(path, flag=-1):\n",
    "    if(flag==-1):\n",
    "        img1 = cv2.imread(path)\n",
    "        img1 = cv2.resize(img1, (300, 300))\n",
    "        return img1\n",
    "    elif(flag==0): #Rescale the Image Only\n",
    "        img1 = cv2.imread(path)[200:850, :800]\n",
    "        return img1\n",
    "    elif(flag==1): #Rescale and resize the image\n",
    "        img1 = cv2.imread(path)\n",
    "        img1=cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        return cv2.resize(img1[320:850, :800], (128, 128))\n",
    "    else:\n",
    "        img1=cv2.imread(path)\n",
    "        img1=cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        p2, p98 = np.percentile(img1, (80, 98))\n",
    "        img_rescale = exposure.rescale_intensity(img1, in_range=(p2, p98))\n",
    "        img2 = rgb2gray(img_rescale[320:850, :800])\n",
    "        #img3 = enhance_contrast(noisy_image, disk(3))\n",
    "\n",
    "        input_img = cv2.resize(img2,(128,128))\n",
    "        return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#530x800\n",
    "img_rows=300\n",
    "img_cols=300\n",
    "num_channel=3\n",
    "num_epoch=20\n",
    "batch_size=32\n",
    "\n",
    "save_folder = 'new-Multi-Layers11-Flag-1-datagen'\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 5\n",
    "\n",
    "labels_name={'Bag0':0,'Bag1':1, 'Bag2':2, 'Bag3':3, 'Bag4':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainSet\n",
      "Bag0 --- 877\n",
      "Bag1 --- 1026\n",
      "Bag2 --- 401\n",
      "Bag3 --- 978\n",
      "Bag4 --- 414\n",
      "*******\n",
      "TestSet\n",
      "Bag0 --- 80\n",
      "Bag1 --- 110\n",
      "Bag2 --- 24\n",
      "Bag3 --- 95\n",
      "Bag4 --- 31\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "def file_count():\n",
    "    minVal,maxVal = 99999999999,-1\n",
    "    min_max = {}\n",
    "    for tv in ['TrainSet', 'TestSet']:\n",
    "        print(tv)\n",
    "        for bag in ['Bag0','Bag1','Bag2','Bag3','Bag4']:\n",
    "            files = os.listdir(os.path.join(os.getcwd(), tv, bag))\n",
    "            minVal = min(minVal,len(files))\n",
    "            maxVal = max(maxVal,len(files))\n",
    "            print(bag,'---', len(files))\n",
    "        min_max[tv] = [minVal,maxVal]\n",
    "        print('*******')\n",
    "    return min_max\n",
    "\n",
    "min_max = file_count()\n",
    "\n",
    "def oversample(maxVal):\n",
    "    for bag in ['Bag0','Bag1','Bag2','Bag3','Bag4']:\n",
    "        files = os.listdir(os.path.join(os.getcwd(), 'TrainSet', bag))\n",
    "        random.shuffle(files)\n",
    "        n_copy = maxVal-len(files)\n",
    "        idx = range(n_copy)\n",
    "        files_copy = [ files[i % len(files)] for i in idx ]\n",
    "        #print('n_copy=',n_copy)\n",
    "        #print('len(files_copy)=',len(files_copy))\n",
    "        for i,file in enumerate(files_copy):\n",
    "            shutil.copyfile(os.path.join(os.getcwd(), 'TrainSet', bag, file),\n",
    "                            os.path.join(os.getcwd(), 'TrainSet', bag, 'copy'+str(i)+'_'+file))\n",
    "\n",
    "def undersample(minVal):\n",
    "    for bag in ['Bag0','Bag1','Bag2','Bag3','Bag4']:\n",
    "        files = os.listdir(os.path.join(os.getcwd(), 'TrainSet', bag))\n",
    "        random.shuffle(files)\n",
    "        n_rm = len(files)-minVal        \n",
    "        files_rm = files[0:n_rm]\n",
    "        #print('n_copy=',n_copy)\n",
    "        #print('len(files_copy)=',len(files_copy))\n",
    "        for file in files_rm:\n",
    "            os.unlink(os.path.join(PATH, 'TrainSet', bag, file))\n",
    "        \n",
    "# oversample training set\n",
    "#maxVal = min_max['train/'][1]\n",
    "#oversample(maxVal,'train/')\n",
    "#minVal = min_max['valid/'][0]\n",
    "#undersample(minVal,'valid/')\n",
    "#file_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[401, 1026]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max['TrainSet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the images of dataset-Bag0\n",
      "\n",
      "1026 80\n",
      "Loading the images of dataset-Bag1\n",
      "\n",
      "1026 110\n",
      "Loading the images of dataset-Bag2\n",
      "\n",
      "1026 24\n",
      "Loading the images of dataset-Bag3\n",
      "\n",
      "1026 95\n",
      "Loading the images of dataset-Bag4\n",
      "\n",
      "1026 31\n"
     ]
    }
   ],
   "source": [
    "balanced = True\n",
    "train_img_data_list=[]\n",
    "test_img_data_list = []\n",
    "train_labels_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "PATH = os.getcwd()\n",
    "train_folder = 'TrainSet'\n",
    "test_folder = 'TestSet'\n",
    "data_dir_list = ['Bag0', 'Bag1', 'Bag2', 'Bag3', 'Bag4']\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    train_img_list = os.listdir(os.path.join(PATH, train_folder, dataset))\n",
    "    if(balanced == True):\n",
    "        random.shuffle(train_img_list)\n",
    "        n_copy = min_max['TrainSet'][1] - len(train_img_list)\n",
    "        idx = range(n_copy)\n",
    "        for i in idx:\n",
    "            train_img_list.append(train_img_list[i % len(train_img_list)])\n",
    "        \n",
    "    test_img_list = os.listdir(os.path.join(PATH, test_folder, dataset))\n",
    "    print ('Loading the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    train_label = labels_name[dataset]\n",
    "    test_label = labels_name[dataset]\n",
    "    for img in train_img_list:\n",
    "        path = os.path.join(PATH, train_folder, dataset, img)\n",
    "        input_img_resize = preprocess_images(path, -1)\n",
    "        train_img_data_list.append(input_img_resize)\n",
    "        train_labels_list.append(train_label)\n",
    "    \n",
    "    for img in test_img_list:\n",
    "        path = os.path.join(PATH, test_folder, dataset, img)\n",
    "        input_img_resize = preprocess_images(path, -1)\n",
    "        test_img_data_list.append(input_img_resize)\n",
    "        test_labels_list.append(test_label)\n",
    "\n",
    "    print(len(train_img_list), len(test_img_list))\n",
    "\n",
    "train_img_data = np.array(train_img_data_list)\n",
    "test_img_data = np.array(test_img_data_list)\n",
    "train_img_data = train_img_data.astype('float32')\n",
    "test_img_data = test_img_data.astype('float32')\n",
    "train_img_data /= 255\n",
    "test_img_data /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels_list)\n",
    "test_labels = np.array(test_labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x,y = shuffle(img_data,Y, random_state=2)\n",
    "# Split the dataset\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "X_train = train_img_data\n",
    "X_test = test_img_data\n",
    "\n",
    "# convert class labels to on-hot encoding\n",
    "y_train = np_utils.to_categorical(train_labels, num_classes)\n",
    "y_test = np_utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_channel==1:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        train_img_data= np.expand_dims(train_img_data, axis=1)\n",
    "        test_img_data= np.expand_dims(test_img_data, axis=1)\n",
    "        \n",
    "    else:\n",
    "        train_img_data= np.expand_dims(train_img_data, axis=4)\n",
    "        test_img_data= np.expand_dims(test_img_data, axis=4)\n",
    "        \n",
    "else:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        train_img_data=np.rollaxis(train_img_data,3,1)\n",
    "        test_img_data=np.rollaxis(test_img_data,3,1)\n",
    "\n",
    "print (train_img_data.shape)\n",
    "print(test_img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.025,\n",
    "        height_shift_range=0.025,\n",
    "        brightness_range=(0.1,0.5),\n",
    "        shear_range=0.5,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='constant',\n",
    "        cval=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=train_img_data[0].shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,(3,3), strides=1, padding='same',input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Conv2D(64, (3, 3), strides=1, padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.005))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.005))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3596))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(2572))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dense(3072))\n",
    "#model.add(Activation('relu'))\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=2e-4),metrics=[\"accuracy\"])\n",
    "\n",
    "# Viewing model_configuration\n",
    "\n",
    "model.summary()\n",
    "model.get_config()\n",
    "model.layers[0].get_config()\n",
    "model.layers[0].input_shape\n",
    "model.layers[0].output_shape\n",
    "model.layers[0].get_weights()\n",
    "np.shape(model.layers[0].get_weights()[0])\n",
    "model.layers[0].trainable\n",
    "\n",
    "# Training\n",
    "hist = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size), \n",
    "                           steps_per_epoch=len(X_train)/batch_size, epochs=num_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(PATH, 'Models', save_folder)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(directory+'/model.json', \"w\") as json_file: #model.json\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(directory+'/weights.h5')\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing losses and accuracy for Train and Validation Sets\n",
    "train_acc=hist.history['acc']\n",
    "train_loss=hist.history['loss']\n",
    "xc=range(num_epoch)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(xc,train_acc)\n",
    "plt.plot(xc,train_loss)\n",
    "plt.xlabel('num of Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Train Acc and Loss')\n",
    "plt.grid(True)\n",
    "plt.legend(['Accuracy','Loss'])\n",
    "\n",
    "plt.savefig(os.path.join(directory, 'AccLossCurve.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "X_test = X_test.reshape(-1, img_rows, img_cols, num_channel)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "test_image = X_test[0:1]\n",
    "print (test_image.shape)\n",
    "\n",
    "print(\"Predicted y class probabilities: \", model.predict(test_image))\n",
    "print(\"Predicted Class: \", model.predict_classes(test_image))\n",
    "print(\"Actual y: \", y_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Printing the confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "#y_pred = model.predict_classes(X_test)\n",
    "#print(y_pred)\n",
    "target_names = ['Bag 0', 'Bag 1', 'Bag 2', 'Bag 3', 'Bag 4']\n",
    "\t\t\t\t\t\n",
    "print(classification_report(np.argmax(y_test,axis=1), y_pred,target_names=target_names))\n",
    "\n",
    "print(confusion_matrix(np.argmax(y_test,axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, binary=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float32') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j],2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    if normalize:\n",
    "        if binary:\n",
    "            plt.savefig(os.path.join(directory, 'normalized_binary_cmatrix.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(directory, 'normalized_confusion_matrix.png'))\n",
    "    else:\n",
    "        if binary:\n",
    "            plt.savefig(os.path.join(directory, 'nn_binary_cmatrix.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(directory, 'nn_confusion_matrix.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=False, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnf_matrix, classes=target_names, normalize=True, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_multilabel_to_binary(cnf, normalize=False):\n",
    "    #Input: Confusion Matrix\n",
    "    correct = [[0,0], [0,0]]\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            if(i in [0, 1]):\n",
    "                if(j not in [0,1]):\n",
    "                    correct[0][1] += cnf[i][j]\n",
    "                else:\n",
    "                    correct[0][0] += cnf[i][j]\n",
    "            else:\n",
    "                if(j not in [2,3,4]):\n",
    "                    correct[1][0] += cnf[i][j]\n",
    "                else:\n",
    "                    correct[1][1] += cnf[i][j]\n",
    "\n",
    "    correct = (np.array(correct))\n",
    "    plot_confusion_matrix(correct, classes=['class 0 (Closed)', 'class 1 (Open)'], normalize=normalize, \n",
    "                          title='Binary Classification', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_multilabel_to_binary(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_multilabel_to_binary(cnf_matrix, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, X_test, y_test, no_classes):\n",
    "\t# Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    \n",
    "    for i in range(no_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    for i in range(no_classes):\n",
    "        plt.plot(fpr[i],tpr[i],label=str(i)+\" auc=\"+str(round(roc_auc[i]*100, 2)))\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.legend(loc=4)\n",
    "    plt.title('Auc Roc Curve')\n",
    "    plt.savefig(os.path.join(directory, 'auc_roc_curve.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(model, X_test, y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the intermediate layer\n",
    "def get_featuremaps(model, layer_idx, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()],[model.layers[layer_idx].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num=7\n",
    "filter_num=0\n",
    "\n",
    "activations = get_featuremaps(model, int(layer_num),test_image)\n",
    "feature_maps = activations[0][0]      \n",
    "\n",
    "if K.image_dim_ordering()=='th':\n",
    "    feature_maps=np.rollaxis((np.rollaxis(feature_maps,2,0)),2,0)\n",
    "\n",
    "num_of_featuremaps=feature_maps.shape[2]\n",
    "fig=plt.figure(figsize=(16,16))\t\n",
    "plt.title(\"featuremaps-layer-{}\".format(layer_num))\n",
    "subplot_num=int(np.ceil(np.sqrt(num_of_featuremaps)))\n",
    "for i in range(int(num_of_featuremaps)):\n",
    "    ax = fig.add_subplot(subplot_num, subplot_num, i+1)\n",
    "    ax.imshow(feature_maps[:,:,i],cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.savefig(os.path.join(directory, \"featuremaps-layer-{}\".format(layer_num)+'.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
